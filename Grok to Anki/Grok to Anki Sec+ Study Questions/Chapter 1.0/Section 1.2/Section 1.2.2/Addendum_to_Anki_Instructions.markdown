# Addendum to Anki Instructions

This addendum provides guidance to supplement the rules in `anki-instructions.txt` for creating Anki-compatible study questions for any subject area, based on iterative feedback and refinements. Version 4 integrates original statements with consolidated statements (September 23, 2025). Version 5 incorporates feedback from question generation for Section 1.1.3 (October 14, 2025), consolidating statements to reduce redundancy and replacing original statements 1, 6, 8, 11, 14–16, 19, and 25 with refined versions (32, 36–38). Version 6 incorporates feedback from question generation for Section 1.2 (September 24, 2025), adding statements 42–56. Version 7 consolidates statements 12+13, 21+31+40+52, 23+32+42, 27+37, 36+46, and 38+50, and resolves conflicts in 26+47 and 23/32/42+44, reducing to 47 statements (September 24, 2025). Version 8 adds 18 general instruction statements based on feedback from question generation for Section 1.2.2 (September 24, 2025). Version 9 consolidates the 18 new statements with existing statements, merging redundancies (e.g., new 1 into 30, new 2 into 5, new 3 into 49), removing duplicates (new 15, original 34, 51), resolving conflicts, and finalizing with 54 statements (September 24, 2025).

1. **Avoid Course-Specific Terms in Questions**: Ensure question text excludes course-specific terms (e.g., specific course names or tools) for reusability, including them only in metadata or explanations, unless they are general subject concepts.
2. **Multiple-Cloze Formats**: For Cloze Deletion (CD) questions, use multiple-cloze structures with part-of-speech hints (e.g., {{c1::term::verb}}, {{c1::identify and record::action and action}}) for complex or compound phrases to enhance granular recall and reduce cognitive load, ensuring hints are specific for compound phrases.
3. **Structured Formatting for Questions**: Ensure tags include Section::X.X.X, topic, question type (e.g., cloze), and instance number for Multiple Choice (MC) questions (e.g., instance-1), with “multiple-correct” for multiple-correct MC questions; assign letter identifiers (A, B, C, D) to match randomized option order in MC “Options” and “OptionsWithCorrect” columns.
4. **Cross-Reference Prior Conversations**: When checking for redundancy, compare proposed questions only against existing questions of the same format (e.g., SA to SA, CD to CD, MC to MC), but broaden the check to include sets of questions testing overlapping concepts or enumerations in prior conversations to ensure distinctness.
5. **Uniform Question Phrasing for MC Instances**: Ensure Multiple Choice (MC) questions for a topic use identical phrasing across instances, varying only QID and options, to prevent phrasing-based cues; use plural phrasing (e.g., “Which are…?”) with “(CHOOSE X)” for multiple-correct questions and singular (e.g., “Which is…?”) for single-correct questions.
6. **Robust Non-Circular MC Explanations**: Ensure Multiple Choice (MC) explanations are robust, avoid circular reasoning, and detail why correct and incorrect options apply, referencing their relevance to the subject.
7. **Consistent Form for MC Distractors**: For Multiple Choice (MC) questions, ensure distractors match the form (single-word or phrase-length) and number (singular, plural, collective) of correct options, using varied, subject-relevant, and plausible distractors (e.g., related concepts, different categories/types, or processes) that avoid synonyms and do not overlap with existing distractors across prior questions. Use full terms instead of initialisms for roles or concepts to avoid overly cuing the correct answer through abbreviation patterns.
8. **Consistent MC Terminology**: Ensure consistent terminology for concepts across Multiple Choice (MC) instances to avoid confusion.
9. **Focus on Core Subject Concepts**: Generate questions only for explicitly stated concepts in the source text. Exclude content related to pedagogical goals, such as learning objectives, course structure, or statements about building foundational knowledge.
10. **Avoid Future Learning Statements**: Do not derive questions from statements about what learners will do or achieve in the future (e.g., "In this module, you will..."). Focus exclusively on concepts currently explained in the text to avoid speculative or procedural content.
11. **Simplify Question Phrasing**: For Simple Answer (SA) and Cloze Deletion (CD) questions, simplify questions and answers to the most concise form, removing superfluous phrases (e.g., redundant context) and avoiding repetition of key answer terms unless necessary, incorporating context into the question to maintain clarity and adhere to the minimum information principle.
12. **Context Cues and Clarity**: Include a subject-specific context cue (e.g., “[Subject]”) in CD Text fields and SA/MC Front/Question fields to prime recall, avoiding extraneous phrases. Ensure clean text by preventing formatting errors and verify CSV fields for proper escaping of commas, pipes, and HTML tags.
13. **Handling Missing Source Details**: For topics not explicitly detailed in the source text, use standard subject definitions or principles to derive atomic, explicit questions aligned with learning outcomes or key terms.
14. **File Naming and Section Specificity**: Name CSV files by section (e.g., SA-X-X-X.csv, CD-X-X-X.csv, MC-X-X-X.csv), including only questions tagged with the relevant section (e.g., Section::X.X.X). Ensure all questions use precise section references in tags and metadata to align with the source text’s structure, correcting discrepancies before finalizing for traceability.
15. **Consistent Terminology**: Use consistent terminology for classifications (e.g., categories vs. types), specifying the classification basis in the question (e.g., “by implementation” vs. “functional types”) to distinguish from related classifications and avoid confusion with secondary concepts.
16. **Emphasize Accessibility**: Shape answers to highlight accessibility for relevant concepts (e.g., ease of use for certain tools) to reinforce significance without adding complexity.
17. **Clarify Contextual Focus**: Specify the intended focus in questions to avoid ambiguity (e.g., clarify scope or aspect), using context cues like “senior executive” or “chief officer” for high-level roles to clarify scope without overly cuing the answer, balancing recall challenge.
18. **Atomic and Scenario-Based SA Questions**: Ensure Simple Answer (SA) questions target a single atomic fact, splitting multi-concept answers into separate questions (e.g., separate questions for each distinct responsibility). For concepts requiring differentiation, create at least two scenario-based SA questions per concept in the format "[Scenario]. What type/category is this?" using distinct, non-obvious scenarios to enhance engagement and nuanced understanding.
19. **Avoid Redundancy in Scenario-Based Questions**: Supplementing Statement 4, for scenario-based Simple Answer (SA) and Multiple Choice (MC) questions, cross-check against all prior questions in the same format to ensure scenarios are distinct, using unique mechanisms or contexts. If a scenario is too similar (e.g., similar mechanisms for the same concept), revise at least twice to find a unique context or mechanism that cues the target concept.
20. **Atomic Cloze Deletion (CD) Questions**: For CD questions, if a single cloze contains multiple concepts (e.g., a compound phrase), break it into multiple clozes with distinct identifiers (e.g., {{c1::}}, {{c2::}}) within one card if the concepts are tightly related and previously learned, treating their interconnectedness as a unified concept to reinforce as a single fact. Use separate cards for distinct or unlearned concepts to maintain atomicity. Provide specific hints (e.g., ‘action or action’) for compound phrases to enhance granularity and reduce cognitive load. For SA questions, split multi-concept answers into separate questions to ensure strict atomicity.
21. **Maintain Identical Correct Options in MC Sets**: Ensure all instances within an MC set have identical correct options, varying only in position/order to prevent phrasing-based cues. Revise any set with inconsistent correct options to standardize answers across instances.
22. **Balanced MC Question Design and Distribution**: For Multiple Choice (MC) questions, ensure balanced distribution of correct option positions across all instances:
    - **4-choose-1**: Target ~25% per option (A, B, C, D; ~6 selections across 24 instances), with 5–7 selections acceptable.
    - **5-choose-2**: Target ~20% per option (A–E; 2–3 selections across 12 selections).
    - **7-choose-4**: Target ~14-15% per option (A–G; 3–4 selections across 24 selections), ensuring no option is at 0%.
    Adjust by swapping correct options with distractors in under/overrepresented positions, preserving content and distractor effectiveness. Defer distribution and randomization analysis until all MC sets are reviewed, presenting a comprehensive report to ensure balance and prevent predictable patterns.
23. **MC Design for Enumerative Concepts**: For Multiple Choice (MC) questions testing enumerations, prefer multi-correct formats (5-choose-2 for 3–5 items, 7-choose-3 for 5+ items, 7-choose-4 for exactly 4 items to create a 7-choose-3 negation) to reinforce enumeration, using 3 instances for simpler concepts and 4 for complex ones. Use 4-choose-1 for single-fact or simpler concepts. Use singular phrasing (e.g., “Which is…?”) for single-correct and plural (e.g., “Which are…? (CHOOSE X)”) for multi-correct questions, with X matching the number of correct answers. The 7-choose-4 format for four-item enumerations must adhere to Statement 22’s distribution targets (~14-15% per option).
24. **Reinforce Key Differentiations Across Formats**: For learning outcomes requiring differentiation, create Simple Answer (SA) questions for definitions, Cloze Deletion (CD) for terms or phrases, and Multiple Choice (MC) for scenario-based application, ensuring scenarios highlight unique aspects without near-identical content. Include all associated elements in CD and MC questions to distinguish from related frameworks (e.g., include distinguishing terms as clozes) and reject sets that overlap too closely with existing questions, proposing new sets with distinct contexts or concepts.
25. **Overlapping Cloze Structures for Enumerations**: For enumerations in CD questions, within the differentiation framework of Statement 24:
    - **≤4 items**: Use overlapping clozes with two items per entry (e.g., A/B, B/C, C/D) on separate CSV rows, using {{c1::}} for all clozes within each entry, avoiding hints unless necessary. For enumerations related to frameworks, ensure alignment with Statement 24 by including distinguishing terms as clozes to avoid interference.
    - **5+ items**: Use three clozes per entry (e.g., A/B/C, B/C/D) on separate rows with {{c1::}} unless distinct aspects require separate identifiers. Avoid creating subset clozes if a comprehensive cloze covering the full enumeration exists, as subsets are inherent to overlapping clozes and can limit understanding.
    - **Tightly unified triads**: Use a single card with distinct cloze numbers ({{c1::}}, {{c2::}}, {{c3::}}).
    - **>6 items**: List up to two items before clozes with ellipses for omissions to reduce cognitive load.
26. **Track and Report Progress**: Maintain a running count of accepted questions per format (SA, CD, MC) and estimate total questions needed (e.g., 20 SA, 8-10 CD, 7-8 MC sets) based on source text complexity and key concepts. Report this count with each question to provide context on progress toward completion.
27. **Wait for Explicit CSV Generation Direction**: Do not generate CSV files until explicitly directed by the user. Maintain a draft of accepted questions in memory and present them only when instructed, ensuring all questions are reviewed and approved beforehand.
28. **Avoid Enumerating Non-Fixed Lists in Clozes**: For Cloze Deletion (CD) questions, do not enumerate examples of open-ended or illustrative lists unless the source text provides a fixed, exhaustive list, to prevent limiting understanding and promote broader recall.
29. **Present All MC Instances as a Single Question for Review**: For Multiple Choice (MC) sets, present all instances (e.g., three instances) together as a single question for evaluation to allow holistic review of consistency and distractor effectiveness.