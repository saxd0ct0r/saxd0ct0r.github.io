# Addendum to Anki Instructions

This addendum provides guidance to supplement the rules in `anki-instructions.txt` for creating Anki-compatible study questions for any subject area, based on iterative feedback and refinements. Version 4 integrates original statements with consolidated statements (September 23, 2025). Version 5 incorporates feedback from question generation for Section 1.1.3 (October 14, 2025), consolidating statements to reduce redundancy and replacing original statements 1, 6, 8, 11, 14–16, 19, and 25 with refined versions (32, 36–38). Version 6 incorporates feedback from question generation for Section 1.2 (September 24, 2025), adding statements 42–56. Version 7 consolidates statements 12+13, 21+31+40+52, 23+32+42, 27+37, 36+46, and 38+50, and resolves conflicts in 26+47 and 23/32/42+44, reducing to 47 statements (September 24, 2025).

2. **Avoid Course-Specific Terms in Questions**: Ensure question text excludes course-specific terms (e.g., specific course names or tools) for reusability, including them only in metadata or explanations, unless they are general subject concepts.
3. **Multiple-Cloze Formats**: Use multiple-cloze structures with part-of-speech hints (e.g., {{c1::term::verb}}) for complex phrases to enhance granular recall.
4. **Structured Formatting for Questions**: Ensure tags include Section::X.X.X, topic, question type (e.g., cloze), and instance number for Multiple Choice (MC) questions (e.g., instance-1), with “multiple-correct” for multiple-correct MC questions; assign letter identifiers (A, B, C, D) to match randomized option order in MC “Options” and “OptionsWithCorrect” columns.
5. **Cross-Reference Prior Conversations**: Check prior conversations, not just previous sections, for duplication, particularly for recurring terms.
7. **Uniform Question Phrasing for MC Instances**: Ensure Multiple Choice (MC) questions for a topic use identical phrasing across instances, varying only QID and options, to prevent phrasing-based cues; use plural phrasing (e.g., “Which are…?”) with “(CHOOSE X)” for multiple-correct questions and singular (e.g., “Which is…?”) for single-correct questions.
9. **Robust Non-Circular MC Explanations**: Ensure Multiple Choice (MC) explanations are robust, avoid circular reasoning, and detail why correct and incorrect options apply, referencing their relevance to the subject.
10. **Consistent Form for MC Distractors**: Ensure Multiple Choice (MC) distractors match the form (single-word or phrase-length) and number (singular, plural, collective) of correct options, shaping correct options to short phrases if single-word distractors are implausible, while being subject-relevant and plausible without overlapping correct answers.
12. **Consistent MC Terminology**: Ensure consistent terminology for concepts across Multiple Choice (MC) instances to avoid confusion.
17. **Focus on Core Subject Concepts**: Generate questions only for explicitly stated concepts in the source text. Exclude content related to pedagogical goals, such as learning objectives, course structure, or statements about building foundational knowledge.
18. **Avoid Future Learning Statements**: Do not derive questions from statements about what learners will do or achieve in the future (e.g., "In this module, you will..."). Focus exclusively on concepts currently explained in the text to avoid speculative or procedural content.
20. **Simplify Question Phrasing**: Remove superfluous phrases from questions to enhance conciseness and clarity, adhering to the minimum information principle. Ensure the question remains focused on the core fact or concept without extraneous context.
28. **Context Cues and Clarity**: Include a subject-specific context cue (e.g., “[Subject]”) in CD Text fields and SA/MC Front/Question fields to prime recall, avoiding extraneous phrases. Ensure clean text by preventing formatting errors and verify CSV fields for proper escaping of commas, pipes, and HTML tags.
29. **Handling Missing Source Details**: For topics not explicitly detailed in the source text, use standard subject definitions or principles to derive atomic, explicit questions aligned with learning outcomes or key terms.
30. **File Naming and Section Specificity**: Name CSV files by section (e.g., SA-X-X-X.csv, CD-X-X-X.csv, MC-X-X-X.csv), including only questions tagged with the relevant section (e.g., Section::X.X.X).
33. **Consistent Terminology**: Use consistent terminology for classifications (e.g., categories vs. types), specifying primary categories or types to indicate hierarchy and avoid confusion with secondary concepts.
34. **Avoid Redundant Wording**: Ensure question phrasing avoids repeating key answer terms unless necessary, rephrasing for clarity (e.g., "What kind of plans?" instead of "What plans?").
35. **Emphasize Accessibility**: Shape answers to highlight accessibility for relevant concepts (e.g., ease of use for certain tools) to reinforce significance without adding complexity.
39. **Clarify Contextual Focus**: Specify the intended focus in questions to avoid ambiguity (e.g., clarify scope or aspect to prevent misinterpretation).
41. **Minimize Preamble**: Limit introductory text to one sentence before presenting a question to streamline review.
42. **Atomic and Scenario-Based SA Questions**: Ensure Simple Answer (SA) questions target a single atomic fact, splitting multi-concept answers into separate questions (e.g., separate questions for each distinct responsibility). For concepts requiring differentiation, create at least two scenario-based SA questions per concept in the format "[Scenario]. What type/category is this?" using distinct, non-obvious scenarios to enhance engagement and nuanced understanding.
43. **Avoid Redundancy in Scenario-Based Questions**: Cross-check scenario-based SA and MC questions against all prior questions (across sections and formats) to ensure scenarios are distinct. If a scenario is too similar (e.g., using similar mechanisms for the same concept), revise at least twice to find a unique context or mechanism that cues the target concept.
44. **Atomic Cloze Deletion (CD) Questions**: For CD questions, if a single cloze contains multiple concepts (e.g., a compound phrase), break it into multiple clozes with distinct identifiers (e.g., {{c1::}}, {{c2::}}) within one card if the concepts are tightly related and previously learned, treating their interconnectedness as a unified concept to reinforce as a single fact. Use separate cards for distinct or unlearned concepts to maintain atomicity. Provide specific hints (e.g., ‘action or action’) for compound phrases to enhance granularity and reduce cognitive load. For SA questions, split multi-concept answers into separate questions to ensure strict atomicity.
45. **Maintain Identical Correct Options in MC Sets**: Ensure all instances within an MC set have identical correct options, varying only in position/order to prevent phrasing-based cues. Revise any set with inconsistent correct options to standardize answers across instances.
46. **Balanced MC Question Design and Distribution**: For MC questions, aim for an even distribution of correct option positions across all instances:
    - **4-choose-1**: Target ~25% per option (A, B, C, D), with 3-4 selections each for 12-16 instances, adjusting by swapping correct options with distractors in underused positions to avoid over- or underrepresentation (e.g., no option at 0%).
    - **5-choose-2**: Target ~20% per option (A-E), ensuring 2-3 selections per option across 12 selections.
    - **7-choose-3**: Target ~14-15% per option (A-G), with 1-2 selections per option across 12-15 selections.
    Use plausible distractors relevant to the subject, matching the form and length of correct answers, avoiding synonyms. Iteratively swap correct options with distractors in underused positions (e.g., A, D, E, G) and document the impact to achieve randomness while preserving question integrity.
47. **MC Design for Enumerative Concepts**: For topics with multiple related items (e.g., categories or types), prefer multi-correct MC formats (5-choose-2 for 3-5 items, 7-choose-3 for 5+ items) to reinforce enumeration, especially for foundational concepts, using 3 instances for simpler concepts and 4 for complex ones. Use 4-choose-1 for single-fact or simpler concepts. Use singular phrasing (e.g., “Which is…?”) for single-correct and plural (e.g., “Which are…? (CHOOSE X)”) for multi-correct questions, with X matching the number of correct answers.
48. **Reinforce Key Differentiations Across Formats**: For learning outcomes requiring differentiation (e.g., similar concepts), create SA questions for definitions, CD for terms or phrases, and MC for scenario-based application, ensuring scenarios highlight unique aspects without near-identical content. Reject MC sets that overlap too closely with existing questions, proposing new sets with distinct contexts or concepts.
49. **Overlapping Cloze Structures for Enumerations**: For enumerations in CD questions:
    - **≤4 items**: Use overlapping clozes with two items per entry (e.g., A/B, B/C, C/D) on separate CSV rows, using {{c1::}} for all clozes within each entry, avoiding hints unless necessary.
    - **5+ items**: Use three clozes per entry (e.g., A/B/C, B/C/D) on separate rows with {{c1::}} unless distinct aspects require separate identifiers.
    - **Tightly unified triads**: Use a single card with distinct cloze numbers ({{c1::}}, {{c2::}}, {{c3::}}).
    - **>6 items**: List up to two items before clozes with ellipses for omissions to reduce cognitive load.
50. **Track and Report Progress**: Maintain a running count of accepted questions per format (SA, CD, MC) and estimate total questions needed (e.g., 20 SA, 8-10 CD, 7-8 MC sets) based on source text complexity and key concepts. Report this count with each question to provide context on progress toward completion.
51. **Wait for Explicit CSV Generation Direction**: Do not generate CSV files until explicitly directed by the user. Maintain a draft of accepted questions in memory and present them only when instructed, ensuring all questions are reviewed and approved beforehand.