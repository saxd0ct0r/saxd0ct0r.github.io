# Addendum to Anki Instructions

This addendum provides additional guidance to supplement the rules in `anki-instructions.txt` for creating Anki-compatible study questions for the CompTIA Security+ exam, based on iterative feedback and refinements. Version 4 integrates original statements with consolidated statements added based on feedback (September 23, 2025).

1. **Atomic SA Answers**: Ensure Simple Answer (SA) answers focus on one atomic fact (e.g., “Handles incident response” for CIRT role), simplifying complex answers by moving additional details to new SA or CD questions, using only details explicitly in the source text.
2. **Avoid Course-Specific Terms in Questions**: Ensure question text excludes course-specific terms (e.g., “CompTIA Security+” or TestOut’s “security simulator”) for reusability, including them only in metadata or explanations, unless they are general cybersecurity concepts.
3. **Multiple-Cloze Formats**: Use multiple-cloze structures with part-of-speech hints (e.g., {{c1::term::verb}}) for complex phrases to enhance granular recall.
4. **Structured Formatting for Questions**: Ensure tags include Section::X.X, topic (e.g., cia), question type (e.g., cloze), and instance number for Multiple Choice (MC) questions (e.g., instance-1), with “multiple-correct” for multiple-correct MC questions; assign letter identifiers (A, B, C, D) to match randomized option order in MC “Options” and “OptionsWithCorrect” columns.
5. **Cross-Reference Prior Conversations**: Check prior conversations, not just previous sections, for duplication, particularly for terms like "availability."
6. **Consistent Cloze Numbering in CD Questions**: Ensure Cloze Deletion (CD) questions use multiple-cloze structures with contextual hints (e.g., {{c1::term::domain}}) for complex phrases, the same cloze number (e.g., {{c1::…}}) for enumerations to cue set size (e.g., CIA triad components), and unique cloze numbers (e.g., {{c1::}}, {{c2::}}) for individual component recall.
7. **Uniform Question Phrasing for MC Instances**: Ensure Multiple Choice (MC) questions for a topic use identical phrasing across instances, varying only QID and options, to prevent phrasing-based cues; use plural phrasing (e.g., “Which are security control categories?”) for multiple-correct questions and singular (e.g., “Which is a security control category?”) for single-correct questions as an exception.
8. **Vary MC Correct Answer Positions**: Ensure Multiple Choice (MC) questions use unique, non-consecutive correct answer positions across instances to prevent pattern cues, allowing consecutive positions sparingly for multiple-correct questions.
9. **Robust Non-Circular MC Explanations**: Ensure Multiple Choice (MC) explanations are robust, avoid circular reasoning (e.g., “CIA triad is correct because it is the CIA triad”), and detail why correct and incorrect options apply, referencing their relevance (e.g., “Confidentiality ensures data access only for authorized users”).
10. **Consistent Form for MC Distractors**: Ensure Multiple Choice (MC) distractors match the form (single-word or phrase-length) and number (singular, plural, collective) of correct options, shaping correct options to short phrases if single-word distractors are implausible, while being cybersecurity-related and plausible without overlapping correct answers.
11. **Vary Option Counts in MC Questions**: Ensure Multiple Choice (MC) questions primarily use four options with one correct answer, infrequently five options with two correct answers, and rarely seven options with three correct answers for triadic concepts (e.g., CIA triad).
12. **Consistent Option Length in MC Questions**: Ensure all Multiple Choice (MC) options, correct and distractors, have consistent length (phrase-length preferred); shape correct options to short phrases if single-word distractors are implausible, and vice versa, to avoid semantic tells (e.g., “Technical safeguard measures” instead of “Technical”).
13. **Consistent MC Terminology**: Ensure consistent terminology for concepts across Multiple Choice (MC) instances (e.g., “Physical access restriction systems” for Physical controls) to avoid confusion.
14. **Strategic Redundancy Across Formats**: Ensure questions reinforce the same concept across SA, CD, and MC formats (e.g., SOC role as SA fact, CD term, and MC selection) to enhance retention without exact duplication.
15. **Overlapping Cloze for Enumerations**: Ensure Cloze Deletion (CD) conversions of enumerative SA questions use an overlapping structure, preferring three items clozed per entry, but two for four-element lists (e.g., A/B, B/C, C/D for IT, HR, legal, marketing) to balance cognitive load and reinforce repetition.
16. **Separate Rows for Overlapping Cloze Entries**: Ensure overlapping cloze questions place each entry on a separate CSV row to create individual Anki cards, testing enumeration subsets (e.g., A/B, B/C, C/D) for incremental learning.
17. **Focus on Core Cybersecurity Concepts**: Generate questions only for explicitly stated cybersecurity concepts (e.g., protecting information and systems) in the source text. Exclude content related to pedagogical goals, such as learning objectives, course structure, or statements about building foundational knowledge (e.g., "building a strong foundation of security concepts").
18. **Avoid Future Learning Statements**: Do not derive questions from statements about what learners will do or achieve in the future (e.g., "In this module, you will..."). Focus exclusively on concepts currently explained in the text to avoid speculative or procedural content.
19. **Prevent Duplication Across Question Formats**: Ensure questions across Simple Answer (SA), Cloze Deletion (CD), and Multiple Choice (MC) formats reinforce the same concept without being near-identical in content or intent. For example, an SA question asking "What does cybersecurity aim to protect?" should not have a CD question with nearly the same phrasing (e.g., "Cybersecurity aims to protect {{c1::information and systems}}") unless it provides distinct learning value.
20. **Simplify Question Phrasing**: Remove superfluous phrases (e.g., "in today’s interconnected world") from questions to enhance conciseness and clarity, adhering to the minimum information principle. Ensure the question remains focused on the core fact or concept without extraneous context.
21. **Present MC Instances Together**: When reviewing Multiple Choice (MC) questions, present all instances of a given question (e.g., instance-1, instance-2) simultaneously for side-by-side comparison to allow evaluation of consistency in phrasing, distractor quality, and correct answer positioning.
22. **Prioritize Atomic and Explicit Facts**: Questions must target atomic facts explicitly stated in the text (e.g., "information and systems" as assets to protect). Avoid inferring or expanding beyond the provided content, especially for abstract or pedagogical statements.
23. **Atomic SA Answers and Scenario-Based Questions**: Ensure Simple Answer (SA) answers focus on a single atomic fact (e.g., “Ensuring data accuracy” for integrity). If an answer is complex, split it into separate SA questions or simplify to the most essential fact, moving details to Cloze Deletion (CD) or Multiple Choice (MC) questions, using standard cybersecurity principles for simplification if the source is vague. For concepts requiring differentiation or application (e.g., integrity vs. non-repudiation, security challenges), create at least two scenario-based SA questions per concept in the format: “This is a scenario. [Question about concept or category]?” (e.g., “A hash function verifies a file. Is this integrity or non-repudiation?”), ensuring scenarios are distinct and non-redundant.
24. **Non-Circular Questions and Explanations**: Ensure questions and MC explanations avoid circular reasoning (e.g., avoid clues like “new risks” for “evolving threats”). Use practical scenarios to imply concepts without restating answers. MC explanations must be robust, non-circular, and clarify why distractors are plausible but incorrect, referencing their cybersecurity relevance (e.g., “Usability focuses on user experience, not information security”).
25. **Overlapping and Non-Overlapping Cloze Structures for Enumerations**: For enumerations, use overlapping CD questions to reinforce recall, listing all items in each entry to clarify set size:
    - **Up to 4 items**: Create entries with 2 clozes (e.g., A/B, B/C, C/D), using {{c1::}} for all clozes within each entry, placed on separate CSV rows for individual Anki cards.
    - **5 or more items**: Use 3 clozes per entry (e.g., A/B/C, B/C/D), using {{c1::}} for all clozes within each entry, placed on separate CSV rows.
    - **More than 6 items**: List up to 2 items before clozes, using ellipses for omissions (e.g., … C, D, {{c1::E}}, {{c1::F}}, {{c1::G}}, H …), placed on separate CSV rows.
   - For enumerations of three items, use a standard, non-overlapping cloze deletion with distinct cloze numbers ({{c1::}}, {{c2::}}, {{c3::}}) for each component, placed on a single CSV row. For tightly unified concepts (e.g., CIA triad), include an additional cloze deletion with all elements using {{c1::}} to cue the number of elements, if the topic’s unified identity merits it, also on a single CSV row.
26. **Multiple Choice Question Design**: Create a sufficient number of Multiple Choice (MC) question instances to prevent learners from memorizing correct answer positions, relying instead on factual content, while balancing against increased review burden. Use three instances for simpler concepts and four for complex or key concepts (e.g., CIA triad, information security). Vary the number of correct options: one correct out of four options (most common), two correct out of five (less frequent), and three correct out of seven (rarest, reserved for triadic concepts like the CIA triad). Randomize correct answer positions for each instance using permutations: 4-choose-1 for single-correct questions (e.g., A, B, C, or D), 5-choose-2 for two-correct questions (e.g., A/B, A/C), or 7-choose-3 for three-correct questions (e.g., B/D/F, B/D/E). For multi-correct questions, randomize the order of correct options within each instance (e.g., Confidentiality/Integrity/Availability vs. Integrity/Availability/Confidentiality). Ensure distractors are plausible, cybersecurity-related, match the form and length of correct options (e.g., phrase-length for phrases), avoid synonyms (e.g., “Administrative” for “Managerial”), and recycle where appropriate with unique combinations to maintain variety. Use identical phrasing across instances, varying only the QID and options. Use singular phrasing (e.g., “What is…?”) for single-correct questions and plural phrasing (e.g., “Which are…?”) with “(CHOOSE X)” for multi-correct questions, where X matches the number of correct answers.
27. **Strategic Redundancy and Format Consistency**: Reinforce key concepts across SA, CD, and MC formats (e.g., SA for definition, CD for term, MC for scenarios) without near-identical content. Ensure MC questions use consistent phrasing across instances, with plural phrasing for multiple-correct questions (e.g., “Which are…?” with “(CHOOSE X)”) and singular for single-correct (e.g., “What is…?”), aligning the CHOOSE prompt with the number of correct answers.
28. **Context Cues and Clarity**: Include “[Cybersecurity]” context cues in CD Text fields and SA/MC Front/Question fields to prime recall, avoiding extraneous phrases (e.g., “in today’s interconnected world”). Ensure clean text by preventing formatting errors (e.g., stray characters like “凛”) and verify CSV fields for proper escaping of commas, pipes, and HTML tags.
29. **Handling Missing Source Details**: For topics not explicitly detailed in the source text (e.g., risk management components), use standard cybersecurity definitions or principles (e.g., identification/assessment/mitigation/monitoring) to derive atomic, explicit questions aligned with Learning Outcomes or Key Terms.
30. **File Naming and Section Specificity**: Name CSV files by section (e.g., SA-1-1.csv, CD-1-1.csv, MC-1-1.csv for Section 1.1), including only questions tagged with the relevant section (e.g., Section::1.1).
31. **Present MC Instances Together**: Present all MC instances (e.g., instance-1, instance-2) simultaneously during review for side-by-side comparison of phrasing, distractor quality, and correct answer positioning.