# Addendum to Anki Instructions

This addendum provides guidance to supplement the rules in `anki-instructions.txt` for creating Anki-compatible study questions for any subject area, based on iterative feedback and refinements. Version 12 (September 25, 2025) consolidates feedback from question generation for Sections 1.1.3, 1.2, 1.2.2, and 2.0–2.5, adding 8 new statements (45–52) and revising Statements 19 and 46 to resolve conflicts. The 66 statements from Version 11 have been consolidated into 52 by merging redundancies (e.g., question phrasing, cloze specificity, scenario design, MC distribution). Conflict resolutions include: (1) aligning Statement 19 with the user-provided template for overlapping clozes with consistent `c1` tags for ≤4 items; (2) revising Statement 46 to prioritize scenario-based MC questions when converting complex SA/CD answers, with non-scenario MC justified in metadata. The statements ensure atomic, precise, and retention-focused questions for spaced-repetition learning.

1. Avoid Course-Specific Terms in Questions: Ensure question text excludes course-specific terms (e.g., specific course names or tools) for reusability, including them only in metadata or explanations, unless they are general subject concepts.
2. Structured Formatting for Questions: Ensure tags include Section::X.X.X, topic, question type (e.g., cloze), and instance number for Multiple Choice (MC) questions (e.g., instance-1), with “multiple-correct” for multiple-correct MC questions; assign letter identifiers (A, B, C, D) to match randomized option order in MC “Options” and “OptionsWithCorrect” columns.
3. Cross-Reference Prior Conversations: When checking for redundancy, compare proposed questions only against existing questions of the same format (e.g., SA to SA, CD to CD, MC to MC), but broaden the check to include sets of questions testing overlapping concepts or enumerations in prior conversations to ensure distinctness.
4. Uniform Question Phrasing for MC Instances: Ensure Multiple Choice (MC) questions for a topic use identical phrasing across instances, varying only QID and options, to prevent phrasing-based cues; use plural phrasing (e.g., “Which are…?”) with “(CHOOSE X)” for multiple-correct questions and singular (e.g., “Which is…?”) for single-correct questions.
5. Robust Non-Circular MC Explanations: Ensure Multiple Choice (MC) explanations are robust, avoid circular reasoning, and detail why correct and incorrect options apply, referencing their relevance to the subject.
6. Consistent Form for MC Distractors: For Multiple Choice (MC) questions, ensure distractors match the form (single-word or phrase-length) and number (singular, plural, collective) of correct options, using varied, subject-relevant, and plausible distractors (e.g., related concepts, different categories/types, or processes) that avoid synonyms and do not overlap with existing distractors across prior questions. Use full terms instead of initialisms for roles or concepts to avoid overly cuing the correct answer through abbreviation patterns.
7. Consistent MC Terminology: Ensure consistent terminology for concepts across Multiple Choice (MC) instances to avoid confusion.
8. Focus on Core Subject Concepts: Generate questions only for explicitly stated concepts in the source text. Exclude content related to pedagogical goals, such as learning objectives, course structure, or statements about building foundational knowledge.
9. Avoid Future Learning Statements: Do not derive questions from statements about what learners will do or achieve in the future (e.g., "In this module, you will..."). Focus exclusively on concepts currently explained in the text to avoid speculative or procedural content.
10. Context Cues and Clarity: Include a subject-specific context cue (e.g., “[Subject]”) in CD Text fields and SA/MC Front/Question fields to prime recall, avoiding extraneous phrases. Ensure clean text by preventing formatting errors and verify CSV fields for proper escaping of commas, pipes, and HTML tags.
11. Handling Missing Source Details: For topics not explicitly detailed in the source text, use standard subject definitions or principles to derive atomic, explicit questions aligned with learning outcomes or key terms.
12. File Naming and Section Specificity: Name CSV files by section (e.g., SA-X-X-X.csv, CD-X-X-X.csv, MC-X-X-X.csv), including only questions tagged with the relevant section (e.g., Section::X.X.X). Ensure all questions use precise section references in tags and metadata to align with the source text’s structure, correcting discrepancies before finalizing for traceability.
13. Emphasize Accessibility: Shape answers to highlight accessibility for relevant concepts (e.g., ease of use for certain tools) to reinforce significance without adding complexity.
14. Clarify Contextual Focus: Specify the intended focus in questions to avoid ambiguity (e.g., clarify scope or aspect), using context cues like “senior executive” or “chief officer” for high-level roles to clarify scope without overly cuing the answer, balancing recall challenge.
15. Maintain Identical Correct Options in MC Sets: Ensure all instances within an MC set have identical correct options, varying only in position/order to prevent phrasing-based cues. Revise any set with inconsistent correct options to standardize answers across instances.
16. Reinforce Key Differentiations Across Formats: For learning outcomes requiring differentiation, create Simple Answer (SA) questions for definitions, Cloze Deletion (CD) for terms or phrases, and Multiple Choice (MC) for scenario-based application, ensuring scenarios highlight unique aspects without near-identical content. Include all associated elements in CD and MC questions to distinguish from related frameworks (e.g., include distinguishing terms as clozes) and reject sets that overlap too closely with existing questions, proposing new sets with distinct contexts or concepts. *[Revised below to align with Statement 46]*
17. Track and Report Progress: Maintain a running count of accepted questions per format (SA, CD, MC) and estimate total questions needed (e.g., 20 SA, 8-10 CD, 7-8 MC sets) based on source text complexity and key concepts. Report this count with each question to provide context on progress toward completion.
18. Ensure Cloze Specificity and Atomicity: For CD questions, ensure clozes are atomic and uniquely fit the sentence using distinguishing details (e.g., "designed to compromise [specific assets]" for malware); use single-cloze for non-enumerative concepts, multiple clozes with part-of-speech hints (e.g., {{c1::deception::tactic}}) for tightly related or semi-related phrases (e.g., possessives like "attacker's" or compound actions) if approved during user feedback with metadata justification, and split unrelated concepts into separate cards to maintain atomicity.
19. *[Revised]* Handle Enumerations in CD Questions: For CD questions testing enumerations, use overlapping clozes on separate cards only for fixed, exhaustive lists from the source. For ≤4 items, create three cards (A/B, B/C, C/D) with consistent `c1` tags (e.g., `{{c1::term::hint}}`) to obscure both elements simultaneously, including all list items in each card’s text (e.g., “This list has elements one, two, three, and four”). For 5+ items, use A/B/C or similar with `c1`, `c2`, etc., tags as needed. Avoid enumerating non-fixed or illustrative lists, using single-cloze for individual items to promote broad recall without limiting scope.
20. Wait for Explicit CSV Generation Direction: Do not generate CSV files until explicitly directed by the user. Maintain a draft of accepted questions in memory and present them only when instructed, ensuring all questions are reviewed and approved beforehand.
21. Present All MC Instances as a Single Question for Review: For Multiple Choice (MC) sets, present all instances (e.g., three instances) together as a single question for evaluation to allow holistic review of consistency and distractor effectiveness.
22. Incorporate Subject Context in Questions: Ensure all SA, CD, and MC questions include a subject-specific prefix (e.g., “[Subject]”) in the question or text field to provide clear context and prime recall, avoiding ambiguity in cross-disciplinary topics.
23. Ensure Distractor Plausibility in MC Questions: For MC questions, select distractors that are plausible within the subject domain but clearly incorrect, ensuring they relate to the topic (e.g., related security terms for cybersecurity) to challenge understanding without introducing unrelated concepts.
24. Avoid Overlapping Concepts in Distractors: In MC questions, avoid distractors that overlap with correct answers in other questions within the same set or prior sets to prevent confusion and interference during review.
25. Validate Question Relevance to Source: Before finalizing questions, cross-check that each SA, CD, and MC question directly corresponds to a concept or fact explicitly stated or implied in the source text, rejecting questions that assume unstated details unless supported by standard domain knowledge (per Statement 11).
26. Use Consistent Question Types for Key Concepts: For each major concept in the source text, generate at least one SA, one CD, and one MC question (or set) to ensure comprehensive coverage across formats, balancing definition recall, term recognition, and application.
27. Prioritize Atomic Facts in SA Questions: Ensure SA questions focus on a single, atomic fact (e.g., one definition or scenario outcome) to minimize cognitive load, splitting complex ideas into multiple questions if necessary.
28. Use Precise, Concise Question and Answer Phrasing: For SA and CD questions, use concise phrasing for questions and answers, avoiding speculative "why" structures (rephrase as "what characteristic/makes [concept] [outcome]?") and redundant context; move necessary context to the question or metadata to maintain clarity and adhere to the minimum information principle.
29. Robust Explanations for MC: In MC questions, provide detailed explanations in the CorrectOptions field, including why each incorrect option is wrong and how it differs from the correct answer, to reinforce understanding and avoid rote memorization.
30. Balance Question Difficulty: Distribute question difficulty across easy (definition-based), moderate (scenario-based), and challenging (differentiation or application) questions within each format to cater to varied learner proficiency while maintaining engagement.
31. Use Standard Definitions for General Terms: When the source text uses general terms (e.g., “various methods”), derive questions using standard subject definitions (e.g., common cybersecurity attacks), ensuring answers are widely accepted and cited in metadata for traceability.
32. Use Distinct Scenarios for Application: For scenario-based SA and MC questions, use unique, practical scenarios (e.g., email vs. phone contexts) that clearly cue the target concept without ambiguity; ensure scenarios are distinct from prior questions in the same format, reflect real-world applications, and avoid redundant mechanisms to enhance nuanced understanding.
33. Incorporate Emotional Engagement in Scenarios: For scenario-based questions, use relatable or vivid examples (e.g., workplace incidents, common user errors) to enhance engagement, ensuring scenarios remain concise and relevant to the concept.
34. Use Incremental Complexity in Question Sets: For complex topics, structure questions to build from basic definitions (SA/CD) to application scenarios (SA/MC) to differentiation (MC), ensuring learners master foundational facts before tackling nuanced applications.
35. Avoid Synonymous Terms in Distractors: In MC questions, avoid distractors that are synonyms or near-synonyms of correct answers to prevent confusion, using terms from related but distinct concepts instead.
36. Split Compound Concepts in SA Questions: For SA questions targeting compound concepts (e.g., multiple roles or outcomes), create separate questions for each distinct element to ensure atomicity and avoid overloading the learner.
37. Standardize Terminology and Articles: Use consistent, precise terminology across SA, CD, and MC questions (e.g., "categories" vs. "types" with clear classification basis), aligning with source text or standard definitions; for clozes preceded by indefinite articles, use "a/n" (e.g., "a/n {{c1::attacker's}}") to neutralize vowel/consonant variations.
38. Provide Clear Metadata in All Questions: Include detailed metadata in SA, CD, and MC questions, specifying question type (e.g., concept, scenario), rationale (purpose and alignment with source), and reference (Section::X.X.X), to ensure traceability and clarity.
39. Avoid Overloading MC Options: In MC questions, limit options to 4 (single-correct) or 5–7 (multi-correct) to prevent cognitive overload, ensuring each option is concise and directly relevant to the question’s focus.
40. Reject Questions with Low-Value Details: Reject SA, CD, or MC questions testing minor or low-impact details from the source text, prioritizing high-value concepts (e.g., core definitions, critical applications) for efficient mastery.
41. Design MC for Enumerations: For MC questions testing enumerations, use multi-correct formats (5-choose-2 for 3–5 items, 7-choose-3 for 5+ items or subsets, 7-choose-4 for exactly 4 items) with 3–4 instances for complex concepts, shuffling correct options across instances; use singular phrasing for single-correct (e.g., "Which is…?") and plural with "(CHOOSE X)" for multi-correct, listing correct options alphabetically in CorrectOptions.
42. Balance MC Distribution and Randomization: For MC questions, ensure balanced distribution of correct option positions (~25% for 4-choose-1, ~14-15% for 5-choose-2 or 7-choose-3/4, non-zero) and unique position sets per instance within a set; swap correct options with plausible distractors in underrepresented positions (e.g., B with G) to avoid duplicates and predictable patterns, deferring analysis until all sets are reviewed.
43. Include Standard Subject Concepts for General Terms: For topics with general terms in the source (e.g., "various strategies"), incorporate standard subject examples (e.g., tailgating for social engineering) per established definitions, limiting to 3–4 items per set to avoid overload, citing rationale in metadata for traceability.
44. Specify Definitions Precisely in SA Questions: For concept definitions in SA questions, use "What term describes [exact description from source]?" to test exact recall, ensuring the answer is a single term or phrase without synonyms, to minimize interference and build foundational vocabulary.
45. Decompose Complex SA Answers into Multiple Atomic Questions: When a Simple Answer (SA) question’s answer is complex (e.g., contains multiple components like identity, action, and outcome), split it into separate SA questions, each testing a single atomic fact (e.g., role, outcome, characteristic). Ensure each question remains concise and uses consistent terminology, per Statement 27 (atomic facts) and Statement 36 (split compound concepts).
46. *[Revised]* Convert Complex SA or CD Answers to MC When Appropriate: If an SA or CD answer is too complex for atomic recall (e.g., multiple components overwhelm learner), convert to an MC question prioritizing scenario-based or application-focused recognition, using plausible distractors from related concepts, per Statement 16. Non-scenario MC questions are allowed only when justified in metadata as necessary to simplify complex concepts, ensuring robust explanations (Statement 5) and plausible distractors (Statement 23).
47. Use Overlapping Clozes with Consistent c1 Tags for Enumerations of ≤4 Items: For Cloze Deletion (CD) questions testing fixed, exhaustive lists of ≤4 items, create three cards (A/B, B/C, C/D) with consistent `c1` tags (e.g., `{{c1::term::hint}}`) to obscure both elements simultaneously on each card. Include all list items in each card’s text to provide context, following the template: “[statement] This list has elements one, two, three, and four.” Ensure part-of-speech hints for clarity, per Statement 19 (overlapping clozes for fixed lists) and Statement 18 (atomicity).
48. Incorporate Standard Subject-Specific Terms When Directed: When instructed to include standard subject-specific terms (e.g., “white hat, black hat, gray hat hackers” in cybersecurity), integrate them into SA, CD, or MC questions, even if not explicitly in the source text, provided they align with standard domain knowledge (Statement 11). Ensure terms are tested distinctly (e.g., separate questions for each term) and use distractors that differentiate related concepts, per Statement 16 (reinforce differentiations).
49. Use Distractors from Non-Target Concepts in MC Questions: For MC questions, select distractors from concepts related to non-target entities or motivations within the same domain (e.g., for a question on nation-state motivations, use motivations of hacktivists or organized crime). This increases plausibility and challenge, avoiding overly obvious distractors like mitigation techniques, per Statement 23 (plausible distractors) and Statement 24 (avoid overlap).
50. Balance Correct Option Positions in MC Questions: Ensure correct option positions in MC questions are evenly distributed (~25% for 4-choose-1, ~14-15% for 7-choose-3/4, non-zero) across all instances in a set, swapping correct options with plausible distractors in underrepresented positions (e.g., F, G in 7-choose-3) to avoid predictability. Verify distribution after all instances are reviewed, per Statement 42 (balanced distribution).
51. Minimize Sequential Correct Options in Multi-Correct MC Questions: In multi-correct MC questions (e.g., 7-choose-3), minimize sequential correct option combinations (e.g., A, B, C) to align with the expected probability (~14.29% or ~1.71 instances for 12 instances). Rearrange correct options to non-consecutive positions (e.g., A, C, E) across instances, ensuring balanced distribution and plausibility, per Statement 42 (balanced distribution).
52. Allow Multi-Cloze with Different Numbered Tags for Semi-Related Concepts: For CD questions testing semi-related concepts (e.g., a term and its characteristic), allow multiple clozes with different numbered tags (e.g., `c1`, `c2`) to obscure elements separately, provided the concepts are tightly linked and justified in metadata, per Statement 18 (cloze specificity). Ensure the question remains concise and uses part-of-speech hints for clarity.