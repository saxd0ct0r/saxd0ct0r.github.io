# Addendum to Anki Instructions

This addendum provides guidance to supplement the rules in `anki-instructions.txt` for creating Anki-compatible study questions for any subject area, based on iterative feedback and refinements. Version 13 (September 29, 2025) consolidates feedback from question generation for Sections 1.1.3, 1.2, 1.2.2, 2.0–2.5, and the conversion of MC-2-1.csv to MCQ-2-1.csv, adding 3 new statements (53–55) to address the "MCQ - Multi-Variant Shuffled" format. The 66 statements from Version 11 were previously consolidated into 52, with revisions to Statements 19 and 46 to resolve conflicts. The new statements ensure tag retention, case-by-case handling of single-correct questions, and distractor consolidation for the new format. No conflicts were identified, as the new statements align with existing principles and address specific conversion needs.

1. Avoid Course-Specific Terms in Questions: Ensure question text excludes course-specific terms (e.g., specific course names or tools) for reusability, including them only in metadata or explanations, unless they are general subject concepts.
2. Structured Formatting for Questions: Ensure tags include Section::X.X.X, topic, question type (e.g., cloze), and instance number for Multiple Choice (MC) questions (e.g., instance-1), with “multiple-correct” for multiple-correct MC questions; assign letter identifiers (A, B, C, D) to match randomized option order in MC “Options” and “OptionsWithCorrect” columns.
3. Cross-Reference Prior Conversations: When checking for redundancy, compare proposed questions only against existing questions of the same format (e.g., SA to SA, CD to CD, MC to MC), but broaden the check to include sets of questions testing overlapping concepts or enumerations in prior conversations to ensure distinctness.
4. Uniform Question Phrasing for MC Instances: Ensure Multiple Choice (MC) questions for a topic use identical phrasing across instances, varying only QID and options, to prevent phrasing-based cues; use plural phrasing (e.g., “Which are…?”) with “(CHOOSE X)” for multiple-correct questions and singular (e.g., “Which is…?”) for single-correct questions.
5. Robust Non-Circular MC Explanations: Ensure Multiple Choice (MC) explanations are robust, avoid circular reasoning, and detail why correct and incorrect options apply, referencing their relevance to the subject.
6. Consistent Form for MC Distractors: For Multiple Choice (MC) questions, ensure distractors match the form (single-word or phrase-length) and number (singular, plural, collective) of correct options, using varied, subject-relevant, and plausible distractors (e.g., related concepts, different categories/types, or processes) that avoid synonyms and do not overlap with existing distractors across prior questions. Use full terms instead of initialisms for roles or concepts to avoid overly cuing the correct answer through abbreviation patterns.
7. Consistent MC Terminology: Ensure consistent terminology for concepts across Multiple Choice (MC) instances to avoid confusion.
8. Focus on Core Subject Concepts: Generate questions only for explicitly stated concepts in the source text. Exclude content related to pedagogical goals, such as learning objectives, course structure, or statements about building foundational knowledge.
9. Avoid Future Learning Statements: Do not derive questions from statements about what learners will do or achieve in the future (e.g., "In this module, you will..."). Focus exclusively on concepts currently explained in the text to avoid speculative or procedural content.
10. Context Cues and Clarity: Include a subject-specific context cue (e.g., “[Subject]”) in CD Text fields and SA/MC Front/Question fields to prime recall, avoiding extraneous phrases. Ensure clean text by preventing formatting errors and verify CSV fields for proper escaping of commas, pipes, and HTML tags.
11. Handling Missing Source Details: For topics not explicitly detailed in the source text, use standard subject definitions or principles to derive atomic, explicit questions aligned with learning outcomes or key terms.
12. File Naming and Section Specificity: Name CSV files by section (e.g., SA-X-X-X.csv, CD-X-X-X.csv, MC-X-X-X.csv), including only questions tagged with the relevant section (e.g., Section::X.X.X). Ensure all questions use precise section references in tags and metadata to align with the source text’s structure, correcting discrepancies before finalizing for traceability.
13. Emphasize Accessibility: Shape answers to highlight accessibility for relevant concepts (e.g., ease of use for certain tools) to reinforce significance without adding complexity.
14. Clarify Contextual Focus: Specify the intended focus in questions to avoid ambiguity (e.g., clarify scope or aspect), using context cues like “senior executive” or “chief officer” for high-level roles to clarify scope without overly cuing the answer, balancing recall challenge.
15. Maintain Identical Correct Options in MC Sets: Ensure all instances within an MC set have identical correct options, varying only in position/order to prevent phrasing-based cues. Revise any set with inconsistent correct options to standardize answers across instances.
16. Reinforce Key Differentiations Across Formats: For learning outcomes requiring differentiation, create Simple Answer (SA) questions for definitions, Cloze Deletion (CD) for terms or phrases, and Multiple Choice (MC) for scenario-based application, ensuring scenarios highlight unique aspects without near-identical content. Include all associated elements in CD and MC questions to distinguish from related frameworks (e.g., include distinguishing terms as clozes) and reject sets that overlap too closely with existing questions, proposing new sets with distinct contexts or concepts.
17. Track and Report Progress: Maintain a running count of accepted questions per format (SA, CD, MC) and estimate total questions needed (e.g., 20 SA, 8-10 CD, 7-8 MC sets) based on source text complexity and key concepts. Report this count with each question to provide context on progress toward completion.
18. Ensure Cloze Specificity and Atomicity: For CD questions, ensure clozes are atomic and uniquely fit the sentence using distinguishing details (e.g., "designed to compromise [specific assets]" for malware); use singular clozes for single facts and multiple clozes for enumerations or tightly related concepts, with part-of-speech hints (e.g., ::term, ::action) for clarity.
19. Use Overlapping Clozes for Fixed Lists: For CD questions testing fixed, exhaustive lists of ≤4 items, create three cards (e.g., A/B, B/C, C/D) with consistent `c1` tags (e.g., `{{c1::term::hint}}`) to obscure two elements simultaneously on each card, including all list items in the text for context, per the user-provided template. For lists >4 items, use singular clozes or split into multiple questions to avoid cognitive overload.
20. Align with Source Text for Cloze Context: Ensure CD questions include sufficient context from the source text in the sentence to make the cloze meaningful, avoiding overly generic phrases that could apply to multiple answers.
21. Use Scenarios for MC Application: For MC questions, prioritize scenario-based questions to test application of concepts (e.g., “A company uses X to achieve Y. What is this?”), ensuring scenarios are distinct and align with source text examples or standard subject scenarios.
22. Avoid Overlapping MC Questions: Ensure MC questions within a set and across sets do not test near-identical concepts or use similar distractors, rejecting or revising questions that risk interference with existing ones.
23. Plausible Distractors for MC: Select MC distractors that are plausible, subject-relevant, and distinct from correct options, ideally from related but non-target concepts (e.g., other roles for a role-based question), avoiding overly obvious or synonymous options.
24. Avoid Distractor Overlap Across Questions: Ensure MC distractors do not repeat across different questions or sets, unless intentional for strategic redundancy, to minimize interference and maintain challenge.
25. Use Standard Subject Terms for Gaps: When the source text uses vague terms (e.g., “various methods”), incorporate standard subject-specific terms (e.g., specific protocols for networking) to create precise questions, limiting to 3–4 terms per question to avoid overload.
26. Ensure Answer Consistency Across Formats: For concepts tested across SA, CD, and MC, ensure answers (e.g., terms, definitions) are consistent in phrasing and scope to avoid confusion, aligning with the source text or standard definitions.
27. Atomic Facts for SA Questions: Ensure SA questions test a single, atomic fact (e.g., one definition, one role) with concise answers (ideally one phrase or term), splitting complex concepts into multiple questions to maintain clarity.
28. Use Mnemonics Sparingly: Incorporate mnemonic aids (e.g., acronyms, vivid imagery) in 1–5% of questions for complex sequences or abstract concepts, ensuring they are simple and source-aligned, per the anki-instructions.txt guidelines.
29. Strategic Redundancy for Key Concepts: For critical concepts, create redundant questions across formats (e.g., SA for definition, CD for term, MC for scenario) with distinct phrasing or context to reinforce retention without overloading.
30. Reference Source Text Precisely: Include precise section references (e.g., Section::X.X.X) in tags and metadata for all questions, citing the source text or standard domain knowledge (e.g., CompTIA objectives) when applicable, to ensure traceability.
31. Avoid Speculative Content: Do not generate questions based on speculative or future-oriented statements (e.g., “learners will explore X”), focusing only on explicitly stated facts or concepts in the source text.
32. Balance Question Types: Aim for a balanced distribution of SA (definitions, facts), CD (terms, lists), and MC (scenarios, recognition) questions to cover comprehension, recall, and application, adjusting based on source text complexity.
33. Verify CSV Formatting: Ensure CSV files use proper escaping for commas, pipes, and HTML tags (e.g., enclose fields with special characters in double quotes) to prevent parsing errors in Anki import.
34. Prioritize High-Impact Concepts: Focus on core concepts, key terms, and critical relationships in the source text, omitting low-value details (e.g., minor examples) to optimize for efficient mastery.
35. Use Context Cues Effectively: Include context cues (e.g., “[Subject]”, role titles) in question text to prime recall without overly cuing the answer, ensuring clarity and specificity.
36. Split Compound Concepts: For complex concepts with multiple components (e.g., role, action, outcome), split into separate SA or CD questions to test each component atomically, avoiding cognitive overload.
37. Standardize Terminology and Articles: Use consistent, precise terminology across SA, CD, and MC questions (e.g., "categories" vs. "types" with clear classification basis), aligning with source text or standard definitions; for clozes preceded by indefinite articles, use "a/n" (e.g., "a/n {{c1::attacker's}}") to neutralize vowel/consonant variations.
38. Provide Clear Metadata in All Questions: Include detailed metadata in SA, CD, and MC questions, specifying question type (e.g., concept, scenario), rationale (purpose and alignment with source), and reference (Section::X.X.X), to ensure traceability and clarity.
39. Avoid Overloading MC Options: In MC questions, limit options to 4 (single-correct) or 5–7 (multi-correct) to prevent cognitive overload, ensuring each option is concise and directly relevant to the question’s focus.
40. Reject Questions with Low-Value Details: Reject SA, CD, or MC questions testing minor or low-impact details from the source text, prioritizing high-value concepts (e.g., core definitions, critical applications) for efficient mastery.
41. Design MC for Enumerations: For MC questions testing enumerations, use multi-correct formats (5-choose-2 for 3–5 items, 7-choose-3 for 5+ items or subsets, 7-choose-4 for exactly 4 items) with 3–4 instances for complex concepts, shuffling correct options across instances; use singular phrasing for single-correct (e.g., "Which is…?") and plural with "(CHOOSE X)" for multi-correct, listing correct options alphabetically in CorrectOptions.
42. Balance MC Distribution and Randomization: For MC questions, ensure balanced distribution of correct option positions (~25% for 4-choose-1, ~14-15% for 5-choose-2 or 7-choose-3/4, non-zero) and unique position sets per instance within a set; swap correct options with plausible distractors in underrepresented positions (e.g., B with G) to avoid duplicates and predictable patterns, deferring analysis until all sets are reviewed.
43. Include Standard Subject Concepts for General Terms: For topics with general terms in the source (e.g., "various strategies"), incorporate standard subject examples (e.g., tailgating for social engineering) per established definitions, limiting to 3–4 items per set to avoid overload, citing rationale in metadata for traceability.
44. Specify Definitions Precisely in SA Questions: For concept definitions in SA questions, use "What term describes [exact description from source]?" to test exact recall, ensuring the answer is a single term or phrase without synonyms, to minimize interference and build foundational vocabulary.
45. Decompose Complex SA Answers into Multiple Atomic Questions: When a Simple Answer (SA) question’s answer is complex (e.g., contains multiple components like identity, action, and outcome), split it into separate SA questions, each testing a single atomic fact (e.g., role, outcome, characteristic). Ensure each question remains concise and uses consistent terminology, per Statement 27 (atomic facts) and Statement 36 (split compound concepts).
46. Convert Complex SA or CD Answers to MC When Appropriate: If an SA or CD answer is too complex for atomic recall (e.g., multiple components overwhelm learner), convert to an MC question prioritizing scenario-based or application-focused recognition, using plausible distractors from related concepts, per Statement 16. Non-scenario MC questions are allowed only when justified in metadata as necessary to simplify complex concepts, ensuring robust explanations (Statement 5) and plausible distractors (Statement 23).
47. Use Overlapping Clozes with Consistent c1 Tags for Enumerations of ≤4 Items: For Cloze Deletion (CD) questions testing fixed, exhaustive lists of ≤4 items, create three cards (A/B, B/C, C/D) with consistent `c1` tags (e.g., `{{c1::term::hint}}`) to obscure both elements simultaneously on each card. Include all list items in each card’s text to provide context, following the template: “[statement] This list has elements one, two, three, and four.” Ensure part-of-speech hints for clarity, per Statement 19 (overlapping clozes for fixed lists) and Statement 18 (atomicity).
48. Incorporate Standard Subject-Specific Terms When Directed: When instructed to include standard subject-specific terms (e.g., “white hat, black hat, gray hat hackers” in cybersecurity), integrate them into SA, CD, or MC questions, even if not explicitly in the source text, provided they align with standard domain knowledge (Statement 11). Ensure terms are tested distinctly (e.g., separate questions for each term) and use distractors that differentiate related concepts, per Statement 16 (reinforce differentiations).
49. Use Distractors from Non-Target Concepts in MC Questions: For MC questions, select distractors from concepts related to non-target entities or motivations within the same domain (e.g., for a question on nation-state motivations, use motivations of hacktivists or organized crime). This increases plausibility and challenge, avoiding overly obvious distractors like mitigation techniques, per Statement 23 (plausible distractors) and Statement 24 (avoid overlap).
50. Balance Correct Option Positions in MC Questions: Ensure correct option positions in MC questions are evenly distributed (~25% for 4-choose-1, ~14-15% for 7-choose-3/4, non-zero) across all instances in a set, swapping correct options with plausible distractors in underrepresented positions (e.g., F, G in 7-choose-3) to avoid predictability. Verify distribution after all instances are reviewed, per Statement 42 (balanced distribution).
51. Minimize Sequential Correct Options in Multi-Correct MC Questions: In multi-correct MC questions (e.g., 7-choose-3), minimize sequential correct option combinations (e.g., A, B, C) to align with the expected probability (~14.29% or ~1.71 instances for 12 instances). Rearrange correct options to non-consecutive positions (e.g., A, C, E) across instances, ensuring balanced distribution and plausibility, per Statement 42 (balanced distribution).
52. Allow Multi-Cloze with Different Numbered Tags for Semi-Related Concepts: For CD questions testing semi-related concepts (e.g., a term and its characteristic), allow multiple clozes with different numbered tags (e.g., `c1`, `c2`) to obscure elements separately, provided the concepts are tightly linked and justified in metadata, per Statement 18 (cloze specificity). Ensure the question remains concise and uses part-of-speech hints for clarity.
53. Retain Original Tags with Minimal Modification: When converting multiple-choice questions to the "MCQ - Multi-Variant Shuffled" format, retain the original tags from the source question, modifying them only to reflect the consolidated note structure (e.g., remove instance-specific tags like “instance-1”) and add “multiple-correct” for questions with multiple correct answers, ensuring traceability and consistency with the source text.
54. Handle Single-Correct Questions Case-by-Case: For questions with fewer than three correct answers in the "MCQ - Multi-Variant Shuffled" format, evaluate case-by-case. Either retain the single correct answer, accepting limitations for Choose-2 and Choose-3 cards (e.g., “Insufficient options” error), or propose minimal additions of closely related correct answers from the source text or standard domain knowledge (e.g., synonyms or related concepts), ensuring alignment and justifying additions in metadata.
55. Consolidate Distractors Across Instances: When consolidating multiple instances of a multiple-choice question into a single "MCQ - Multi-Variant Shuffled" note, combine distractors from all instances to meet the minimum requirement of four distractors, ensuring they are plausible, non-overlapping, and aligned with the source text to maintain challenge and avoid redundancy.