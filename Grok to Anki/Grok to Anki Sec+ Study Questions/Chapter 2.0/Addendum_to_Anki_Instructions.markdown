# Addendum to Anki Instructions

This addendum provides guidance to supplement the rules in `anki-instructions.txt` for creating Anki-compatible study questions for any subject area, based on iterative feedback and refinements. Version 11 (September 25, 2025) consolidates feedback from question generation for Section 2.0 (Threats, Vulnerabilities, and Mitigations), reducing 66 statements to 54 by merging redundancies (e.g., question phrasing, cloze specificity, scenario design, MC distribution) and resolving conflicts. Conflict resolutions include: (1) restricting overlapping cloze enumerations to fixed, exhaustive lists, using single-cloze for non-fixed lists (Statement 19); (2) allowing multiple clozes for semi-related concepts on one card with user approval during feedback, justified in metadata (Statement 18). It builds on Version 9 (September 24, 2025), incorporating feedback from Sections 1.1.3, 1.2, 1.2.2, and 2.0. The statements ensure atomic, precise, and retention-focused questions for spaced-repetition learning.

1. Avoid Course-Specific Terms in Questions: Ensure question text excludes course-specific terms (e.g., specific course names or tools) for reusability, including them only in metadata or explanations, unless they are general subject concepts.
2. Structured Formatting for Questions: Ensure tags include Section::X.X.X, topic, question type (e.g., cloze), and instance number for Multiple Choice (MC) questions (e.g., instance-1), with “multiple-correct” for multiple-correct MC questions; assign letter identifiers (A, B, C, D) to match randomized option order in MC “Options” and “OptionsWithCorrect” columns.
3. Cross-Reference Prior Conversations: When checking for redundancy, compare proposed questions only against existing questions of the same format (e.g., SA to SA, CD to CD, MC to MC), but broaden the check to include sets of questions testing overlapping concepts or enumerations in prior conversations to ensure distinctness.
4. Uniform Question Phrasing for MC Instances: Ensure Multiple Choice (MC) questions for a topic use identical phrasing across instances, varying only QID and options, to prevent phrasing-based cues; use plural phrasing (e.g., “Which are…?”) with “(CHOOSE X)” for multiple-correct questions and singular (e.g., “Which is…?”) for single-correct questions.
5. Robust Non-Circular MC Explanations: Ensure Multiple Choice (MC) explanations are robust, avoid circular reasoning, and detail why correct and incorrect options apply, referencing their relevance to the subject.
6. Consistent Form for MC Distractors: For Multiple Choice (MC) questions, ensure distractors match the form (single-word or phrase-length) and number (singular, plural, collective) of correct options, using varied, subject-relevant, and plausible distractors (e.g., related concepts, different categories/types, or processes) that avoid synonyms and do not overlap with existing distractors across prior questions. Use full terms instead of initialisms for roles or concepts to avoid overly cuing the correct answer through abbreviation patterns.
7. Consistent MC Terminology: Ensure consistent terminology for concepts across Multiple Choice (MC) instances to avoid confusion.
8. Focus on Core Subject Concepts: Generate questions only for explicitly stated concepts in the source text. Exclude content related to pedagogical goals, such as learning objectives, course structure, or statements about building foundational knowledge.
9. Avoid Future Learning Statements: Do not derive questions from statements about what learners will do or achieve in the future (e.g., "In this module, you will..."). Focus exclusively on concepts currently explained in the text to avoid speculative or procedural content.
10. Context Cues and Clarity: Include a subject-specific context cue (e.g., “[Subject]”) in CD Text fields and SA/MC Front/Question fields to prime recall, avoiding extraneous phrases. Ensure clean text by preventing formatting errors and verify CSV fields for proper escaping of commas, pipes, and HTML tags.
11. Handling Missing Source Details: For topics not explicitly detailed in the source text, use standard subject definitions or principles to derive atomic, explicit questions aligned with learning outcomes or key terms.
12. File Naming and Section Specificity: Name CSV files by section (e.g., SA-X-X-X.csv, CD-X-X-X.csv, MC-X-X-X.csv), including only questions tagged with the relevant section (e.g., Section::X.X.X). Ensure all questions use precise section references in tags and metadata to align with the source text’s structure, correcting discrepancies before finalizing for traceability.
13. Emphasize Accessibility: Shape answers to highlight accessibility for relevant concepts (e.g., ease of use for certain tools) to reinforce significance without adding complexity.
14. Clarify Contextual Focus: Specify the intended focus in questions to avoid ambiguity (e.g., clarify scope or aspect), using context cues like “senior executive” or “chief officer” for high-level roles to clarify scope without overly cuing the answer, balancing recall challenge.
15. Maintain Identical Correct Options in MC Sets: Ensure all instances within an MC set have identical correct options, varying only in position/order to prevent phrasing-based cues. Revise any set with inconsistent correct options to standardize answers across instances.
16. Reinforce Key Differentiations Across Formats: For learning outcomes requiring differentiation, create Simple Answer (SA) questions for definitions, Cloze Deletion (CD) for terms or phrases, and Multiple Choice (MC) for scenario-based application, ensuring scenarios highlight unique aspects without near-identical content. Include all associated elements in CD and MC questions to distinguish from related frameworks (e.g., include distinguishing terms as clozes) and reject sets that overlap too closely with existing questions, proposing new sets with distinct contexts or concepts.
17. Track and Report Progress: Maintain a running count of accepted questions per format (SA, CD, MC) and estimate total questions needed (e.g., 20 SA, 8-10 CD, 7-8 MC sets) based on source text complexity and key concepts. Report this count with each question to provide context on progress toward completion.
18. *Ensure Cloze Specificity and Atomicity: For CD questions, ensure clozes are atomic and uniquely fit the sentence using distinguishing details (e.g., "designed to compromise [specific assets]" for malware); use single-cloze for non-enumerative concepts, multiple clozes with part-of-speech hints (e.g., {{c1::deception::tactic}}) for tightly related or semi-related phrases (e.g., possessives like "attacker's" or compound actions) if approved during user feedback with metadata justification, and split unrelated concepts into separate cards to maintain atomicity. [Consolidates 20, 49, 60, 61, 63]
19. *Handle Enumerations in CD Questions: For CD questions testing enumerations, use overlapping clozes (e.g., A/B, B/C for ≤4 items; A/B/C for 5+ items) on separate cards only for fixed, exhaustive lists from the source; avoid enumerating non-fixed or illustrative lists, using single-cloze for individual items to promote broad recall without limiting scope. [Consolidates 25, 28, 59]
20. Wait for Explicit CSV Generation Direction: Do not generate CSV files until explicitly directed by the user. Maintain a draft of accepted questions in memory and present them only when instructed, ensuring all questions are reviewed and approved beforehand.
21. Present All MC Instances as a Single Question for Review: For Multiple Choice (MC) sets, present all instances (e.g., three instances) together as a single question for evaluation to allow holistic review of consistency and distractor effectiveness.
22. Incorporate Subject Context in Questions: Ensure all SA, CD, and MC questions include a subject-specific prefix (e.g., “[Subject]”) in the question or text field to provide clear context and prime recall, avoiding ambiguity in cross-disciplinary topics.
23. Ensure Distractor Plausibility in MC Questions: For MC questions, select distractors that are plausible within the subject domain but clearly incorrect, ensuring they relate to the topic (e.g., related security terms for cybersecurity) to challenge understanding without introducing unrelated concepts.
24. Avoid Overlapping Concepts in Distractors: In MC questions, avoid distractors that overlap with correct answers in other questions within the same set or prior sets to prevent confusion and interference during review.
25. Validate Question Relevance to Source: Before finalizing questions, cross-check that each SA, CD, and MC question directly corresponds to a concept or fact explicitly stated or implied in the source text, rejecting questions that assume unstated details unless supported by standard domain knowledge (per Statement 11).
26. Use Consistent Question Types for Key Concepts: For each major concept in the source text, generate at least one SA, one CD, and one MC question (or set) to ensure comprehensive coverage across formats, balancing definition recall, term recognition, and application.
27. Prioritize Atomic Facts in SA Questions: Ensure SA questions focus on a single, atomic fact (e.g., one definition or scenario outcome) to minimize cognitive load, splitting complex ideas into multiple questions if necessary.
28. *Use Precise, Concise Question and Answer Phrasing: For SA and CD questions, use concise phrasing for questions and answers, avoiding speculative "why" structures (rephrase as "what characteristic/makes [concept] [outcome]?") and redundant context; move necessary context to the question or metadata to maintain clarity and adhere to the minimum information principle. [Consolidates 11, 39, 55, 56]
29. Robust Explanations for MC: In MC questions, provide detailed explanations in the CorrectOptions field, including why each incorrect option is wrong and how it differs from the correct answer, to reinforce understanding and avoid rote memorization.
30. Balance Question Difficulty: Distribute question difficulty across easy (definition-based), moderate (scenario-based), and challenging (differentiation or application) questions within each format to cater to varied learner proficiency while maintaining engagement.
31. Use Standard Definitions for General Terms: When the source text uses general terms (e.g., “various methods”), derive questions using standard subject definitions (e.g., common cybersecurity attacks), ensuring answers are widely accepted and cited in metadata for traceability.
32. *Use Distinct Scenarios for Application: For scenario-based SA and MC questions, use unique, practical scenarios (e.g., email vs. phone contexts) that clearly cue the target concept without ambiguity; ensure scenarios are distinct from prior questions in the same format, reflect real-world applications, and avoid redundant mechanisms to enhance nuanced understanding. [Consolidates 18, 19, 32, 37, 42, 48, 58]
33. Incorporate Emotional Engagement in Scenarios: For scenario-based questions, use relatable or vivid examples (e.g., workplace incidents, common user errors) to enhance engagement, ensuring scenarios remain concise and relevant to the concept.
34. Use Incremental Complexity in Question Sets: For complex topics, structure questions to build from basic definitions (SA/CD) to application scenarios (SA/MC) to differentiation (MC), ensuring learners master foundational facts before tackling nuanced applications.
35. Avoid Synonymous Terms in Distractors: In MC questions, avoid distractors that are synonyms or near-synonyms of correct answers to prevent confusion, using terms from related but distinct concepts instead.
36. Split Compound Concepts in SA Questions: For SA questions targeting compound concepts (e.g., multiple roles or outcomes), create separate questions for each distinct element to ensure atomicity and avoid overloading the learner.
37. *Standardize Terminology and Articles: Use consistent, precise terminology across SA, CD, and MC questions (e.g., "categories" vs. "types" with clear classification basis), aligning with source text or standard definitions; for clozes preceded by indefinite articles, use "a/n" (e.g., "a/n {{c1::attacker's}}") to neutralize vowel/consonant variations. [Consolidates 15, 47, 53, 62]
38. Provide Clear Metadata in All Questions: Include detailed metadata in SA, CD, and MC questions, specifying question type (e.g., concept, scenario), rationale (purpose and alignment with source), and reference (Section::X.X.X), to ensure traceability and clarity.
39. Avoid Overloading MC Options: In MC questions, limit options to 4 (single-correct) or 5–7 (multi-correct) to prevent cognitive overload, ensuring each option is concise and directly relevant to the question’s focus.
40. Reject Questions with Low-Value Details: Reject SA, CD, or MC questions testing minor or low-impact details from the source text, prioritizing high-value concepts (e.g., core definitions, critical applications) for efficient mastery.
41. *Design MC for Enumerations: For MC questions testing enumerations, use multi-correct formats (5-choose-2 for 3–5 items, 7-choose-3 for 5+ items or subsets, 7-choose-4 for exactly 4 items) with 3–4 instances for complex concepts, shuffling correct options across instances; use singular phrasing for single-correct (e.g., "Which is…?") and plural with "(CHOOSE X)" for multi-correct, listing correct options alphabetically in CorrectOptions. [Consolidates 23, parts of 65]
42. *Balance MC Distribution and Randomization: For MC questions, ensure balanced distribution of correct option positions (~25% for 4-choose-1, ~20% for 5-choose-2, ~14-15% for 7-choose-3/4, non-zero) and unique position sets per instance within a set; swap correct options with plausible distractors in underrepresented positions (e.g., B with G) to avoid duplicates and predictable patterns, deferring analysis until all sets are reviewed. [Consolidates 22, 62, 66]
43. *Include Standard Subject Concepts for General Terms: For topics with general terms in the source (e.g., "various strategies"), incorporate standard subject examples (e.g., tailgating for social engineering) per established definitions, limiting to 3–4 items per set to avoid overload, citing rationale in metadata for traceability. [Consolidates 41, 64]
44. *Specify Definitions Precisely in SA Questions: For concept definitions in SA questions, use "What term describes [exact description from source]?" to test exact recall, ensuring the answer is a single term or phrase without synonyms, to minimize interference and build foundational vocabulary. [Consolidates 57]