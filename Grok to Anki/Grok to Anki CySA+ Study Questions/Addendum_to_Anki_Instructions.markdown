# Addendum to Anki Instructions

This addendum supplements the rules in `anki-instructions.txt` for creating Anki-compatible study questions for any subject area. Version 16 (September 30, 2025) adds 4 new statements (58–61) to the 57 from Version 15, addressing MCQ format conversion and its adoption as the default for all future Multiple Choice questions for Section 2.0, ensuring atomic, precise, and retention-focused questions for spaced-repetition learning.

1. Avoid Course-Specific Terms in Questions: Ensure question text excludes course-specific terms (e.g., specific course names or tools) for reusability, including them only in metadata or explanations, unless they are general subject concepts.
2. Structured Formatting for Questions: Ensure tags include Section::X.X.X, topic, question type (e.g., cloze), and instance number for Multiple Choice (MC) questions (e.g., instance-1), with “multiple-correct” for multiple-correct MC questions; assign letter identifiers (A, B, C, D) to match randomized option order in MC “Options” and “OptionsWithCorrect” columns.
3. Cross-Reference Prior Conversations: When checking for redundancy, compare proposed questions only against existing questions of the same format (e.g., SA to SA, CD to CD, MC to MC), but broaden the check to include sets of questions testing overlapping concepts or enumerations in prior conversations to ensure distinctness.
4. Uniform Question Phrasing for MC Instances: Ensure Multiple Choice (MC) questions for a topic use identical phrasing across instances, varying only QID and options, to prevent phrasing-based cues; use plural phrasing (e.g., “Which are…?”) with “(CHOOSE X)” for multiple-correct questions and singular (e.g., “Which is…?”) for single-correct questions.
5. Robust Non-Circular MC Explanations: Ensure Multiple Choice (MC) explanations are robust, avoid circular reasoning, and detail why correct and incorrect options apply, referencing their relevance to the subject.
6. Consistent Form for MC Distractors: For Multiple Choice (MC) questions, ensure distractors match the form (single-word or phrase-length) and number (singular, plural, collective) of correct options, using varied, subject-relevant, and plausible distractors (e.g., related concepts, different categories/types, or processes) that avoid synonyms and do not overlap with existing distractors across prior questions. Use full terms instead of initialisms for roles or concepts to avoid overly cuing the correct answer through abbreviation patterns.
7. Consistent MC Terminology: Ensure consistent terminology for concepts across Multiple Choice (MC) instances to avoid confusion.
8. Focus on Core Subject Concepts: Generate questions only for explicitly stated concepts in the source text. Exclude content related to pedagogical goals, such as learning objectives, course structure, or statements about building foundational knowledge.
9. Avoid Future Learning Statements: Do not derive questions from statements about what learners will do or achieve in the future (e.g., "In this module, you will..."). Focus exclusively on concepts currently explained in the text to avoid speculative or procedural content.
10. Context Cues and Clarity: Include a subject-specific context cue (e.g., “[Subject]”) in CD Text fields and SA/MC Front/Question fields to prime recall, avoiding extraneous phrases. Ensure clean text by preventing formatting errors and verify CSV fields for proper escaping of commas, pipes, and HTML tags.
11. Handling Missing Source Details: For topics not explicitly detailed in the source text, use standard subject definitions or principles to derive atomic, explicit questions aligned with learning outcomes or key terms.
12. File Naming and Section Specificity: Name CSV files by section (e.g., SA-X-X-X.csv, CD-X-X-X.csv, MC-X-X-X.csv), including only questions tagged with the relevant section (e.g., Section::X.X.X). Ensure all questions use precise section references in tags and metadata to align with the source text’s structure, correcting discrepancies before finalizing for traceability.
13. Emphasize Accessibility: Shape answers to highlight accessibility for relevant concepts (e.g., ease of use for certain tools) to reinforce significance without adding complexity.
14. Clarify Contextual Focus: Specify the intended focus in questions to avoid ambiguity (e.g., clarify scope or aspect), using context cues like “senior executive” or “chief officer” for high-level roles to clarify scope without overly cuing the answer, balancing recall challenge.
15. Maintain Identical Correct Options in MC Sets: Ensure all instances within an MC set have identical correct options, varying only in position/order to prevent phrasing-based cues. Revise any set with inconsistent correct options to standardize answers across instances.
16. Reinforce Key Differentiations Across Formats: For learning outcomes requiring differentiation, create Simple Answer (SA) questions for definitions, Cloze Deletion (CD) for terms or phrases, and Multiple Choice (MC) for scenario-based application, ensuring scenarios highlight unique aspects without near-identical content. Include all associated elements in CD and MC questions to distinguish from related frameworks (e.g., include distinguishing terms as clozes) and reject sets that overlap too closely with existing questions, proposing new sets with distinct contexts or concepts.
17. Track and Report Progress: Maintain a running count of accepted questions per format (SA, CD, MC) and estimate total questions needed (e.g., 20 SA, 8-10 CD, 7-8 MC sets) based on source text complexity and key concepts.
18. Cloze Specificity: For Cloze Deletion (CD) questions, ensure each cloze targets a single, atomic fact (e.g., a term, number, or concept) with clear part-of-speech hints (e.g., `::term`, `::number`) to reduce ambiguity, avoiding overlap between clozes within the same sentence unless testing semi-related concepts (Statement 52).
19. Overlapping Clozes for Fixed Lists: For enumerations of ≤4 items in Cloze Deletion (CD) questions, create three cards (A/B, B/C, C/D) with consistent `c1` tags (e.g., `{{c1::term::hint}}`) to obscure both elements simultaneously, including all list items in each card’s text for context. Ensure part-of-speech hints for clarity, aligning with the provided template for overlapping clozes.
20. Balanced Distribution of Correct Options: For Multiple Choice (MC) questions, ensure even distribution of correct option positions (~25% for 4-choose-1, ~14-15% for 5-choose-2 or 7-choose-3/4, non-zero) across instances in a set, swapping correct options with plausible distractors in underrepresented positions to avoid predictable patterns.
21. Scenario-Based MC Questions: Prioritize scenario-based Multiple Choice (MC) questions for application-focused concepts, ensuring scenarios are realistic, subject-relevant, and highlight unique aspects of the correct option, per Statement 16 (reinforce differentiations).
22. Avoid Overlap in MC Sets: Ensure Multiple Choice (MC) sets do not overlap in content or focus with existing questions from prior conversations, rejecting sets that test nearly identical concepts unless distinct contexts or applications are provided.
23. Plausible Distractors: For Multiple Choice (MC) questions, use distractors that are plausible within the subject domain, derived from related but distinct concepts, processes, or entities, avoiding overly obvious or irrelevant options.
24. Avoid Distractor Overlap: Ensure distractors in Multiple Choice (MC) questions do not overlap with correct options from other questions or instances within the same set, maintaining distinctness to prevent confusion.
25. Minimize Redundancy in Question Sets: When generating multiple question formats (SA, CD, MC) for a topic, ensure each format tests a distinct aspect (e.g., SA for definition, CD for term recall, MC for application) to avoid redundancy, per Statement 16 (reinforce differentiations).
26. Use Standard Subject Terms: When source text uses general terms (e.g., “various threats”), incorporate standard subject-specific terms (e.g., phishing, malware for cybersecurity) in questions, limiting to 3–4 items to avoid overload, per Statement 11 (handling missing source details).
27. Atomic Facts in SA Questions: Ensure Simple Answer (SA) questions target a single, atomic fact (e.g., a term, role, or outcome), avoiding compound answers that require multiple components unless split into separate questions (Statement 45).
28. Consistent Terminology Across Formats: Use identical terminology for concepts across Simple Answer (SA), Cloze Deletion (CD), and Multiple Choice (MC) questions to prevent confusion, aligning with Statement 7 (consistent MC terminology).
29. Visual Cues in Questions: For subjects with visual components (e.g., diagrams, charts), include questions that evoke visual imagery (e.g., “What component is shown in [diagram]?”) and flag for image selection/generation, per the anki-instructions.txt guidelines.
30. Mnemonic Aids for Complex Concepts: For 1–5% of questions involving complex sequences or abstract concepts, suggest simple mnemonic aids (e.g., acronyms, mind maps) to enhance retention, per the anki-instructions.txt guidelines.
31. Ordered Enumerations: Prefer ordered enumerations (e.g., alphabetical, numerical) in Cloze Deletion (CD) questions for lists to reduce cognitive load, breaking large lists into overlapping clozes for clarity, per the anki-instructions.txt guidelines.
32. Context Cues for Clarity: Include context cues (e.g., “[Cybersecurity]”, “net:”) in question text to prime recall and reduce ambiguity, avoiding extraneous phrases that increase cognitive load, per Statement 10 (context cues and clarity).
33. Strategic Redundancy for Key Facts: Use bidirectional pairs or rephrased questions for high-impact facts (e.g., term-to-definition and definition-to-term) in Simple Answer (SA) or Cloze Deletion (CD) formats, ensuring atomicity, per the anki-instructions.txt guidelines.
34. Source References in Metadata: Include brief source references (e.g., document title, section) in metadata for traceability, except for well-established knowledge, noting conflicting sources if applicable, per the anki-instructions.txt guidelines.
35. Volatile Elements in Questions: Add date stamps or version tags (e.g., “[2023]”) for questions involving statistics or technology details that may change, avoiding stamp memorization unless tracking changes is the focus, per the anki-instructions.txt guidelines.
36. Split Compound Concepts: For complex concepts involving multiple components (e.g., role, action, outcome), split into separate Simple Answer (SA) or Cloze Deletion (CD) questions, each targeting a single atomic fact, per Statement 27 (atomic facts in SA questions).
37. High-Impact Focus: Prioritize questions on core concepts, key facts, and critical relationships, omitting low-value details to optimize retention, per the anki-instructions.txt guidelines.
38. CSV Escaping: Ensure proper CSV escaping for commas, pipes, and HTML tags in all fields to prevent parsing errors, per Statement 10 (context cues and clarity).
39. Certification Focus: For certification-related content (e.g., CySA+), ensure questions target core concepts directly tied to the certification, avoiding peripheral elements like labs or broad objectives unless explicitly linked, per Statement 53 (focus on certification-relevant concepts).
40. Avoid One-to-Many Answers: Avoid questions with one-to-many answers (e.g., “What are all possible…?”) unless broken into atomic questions or designed as multi-correct Multiple Choice (MC) questions with clear limits (e.g., “CHOOSE TWO”), per the anki-instructions.txt guidelines.
41. Assume Prior Knowledge: Assume knowledge from prior sections (per section numbering) when generating questions, avoiding duplication of content covered in earlier sections, per the anki-instructions.txt guidelines.
42. Verify MC Option Distribution: After generating Multiple Choice (MC) sets, verify even distribution of correct option positions (~25% for 4-choose-1, ~14-15% for 5-choose-2 or 7-choose-3/4, non-zero) and unique position sets per instance within a set; swap correct options with plausible distractors in underrepresented positions (e.g., B with G) to avoid duplicates and predictable patterns, deferring analysis until all sets are reviewed.
43. Include Standard Subject Concepts for General Terms: For topics with general terms in the source (e.g., "various strategies"), incorporate standard subject examples (e.g., tailgating for social engineering) per established definitions, limiting to 3–4 items per set to avoid overload, citing rationale in metadata for traceability.
44. Specify Definitions Precisely in SA Questions: For concept definitions in SA questions, use "What term describes [exact description from source]?" to test exact recall, ensuring the answer is a single term or phrase without synonyms, to minimize interference and build foundational vocabulary.
45. Decompose Complex SA Answers into Multiple Atomic Questions: When a Simple Answer (SA) question’s answer is complex (e.g., contains multiple components like identity, action, and outcome), split it into separate SA questions, each testing a single atomic fact (e.g., role, outcome, characteristic). Ensure each question remains concise and uses consistent terminology, per Statement 27 (atomic facts) and Statement 36 (split compound concepts).
46. *[Revised]* Convert Complex SA or CD Answers to MC When Appropriate: If an SA or CD answer is too complex for atomic recall (e.g., multiple components overwhelm learner), convert to an MC question prioritizing scenario-based or application-focused recognition, using plausible distractors from related concepts, per Statement 16. Non-scenario MC questions are allowed only when justified in metadata as necessary to simplify complex concepts, ensuring robust explanations (Statement 5) and plausible distractors (Statement 23).
47. Use Overlapping Clozes with Consistent c1 Tags for Enumerations of ≤4 Items: For Cloze Deletion (CD) questions testing fixed, exhaustive lists of ≤4 items, create three cards (A/B, B/C, C/D) with consistent `c1` tags (e.g., `{{c1::term::hint}}`) to obscure both elements simultaneously on each card. Include all list items in each card’s text to provide context, following the template: “[statement] This list has elements one, two, three, and four.” Ensure part-of-speech hints for clarity, per Statement 19 (overlapping clozes for fixed lists) and Statement 18 (atomicity).
48. Incorporate Standard Subject-Specific Terms When Directed: When instructed to include standard subject-specific terms (e.g., “white hat, black hat, gray hat hackers” in cybersecurity), integrate them into SA, CD, or MC questions, even if not explicitly in the source text, provided they align with standard domain knowledge (Statement 11). Ensure terms are tested distinctly (e.g., separate questions for each term) and use distractors that differentiate related concepts, per Statement 16 (reinforce differentiations).
49. Use Distractors from Non-Target Concepts in MC Questions: For MC questions, select distractors from concepts related to non-target entities or motivations within the same domain (e.g., for a question on nation-state motivations, use motivations of hacktivists or organized crime). This increases plausibility and challenge, avoiding overly obvious distractors like mitigation techniques, per Statement 23 (plausible distractors) and Statement 24 (avoid overlap).
50. Balance Correct Option Positions in MC Questions: Ensure correct option positions in MC questions are evenly distributed (~25% for 4-choose-1, ~14-15% for 7-choose-3/4, non-zero) across all instances in a set, swapping correct options with plausible distractors in underrepresented positions (e.g., F, G in 7-choose-3) to avoid predictability. Verify distribution after all instances are reviewed, per Statement 42 (balanced distribution).
51. Minimize Sequential Correct Options in Multi-Correct MC Questions: In multi-correct MC questions (e.g., 7-choose-3), minimize sequential correct option combinations (e.g., A, B, C) to align with the expected probability (~14.29% or ~1.71 instances for 12 instances). Rearrange correct options to non-consecutive positions (e.g., A, C, E) across instances, ensuring balanced distribution and plausibility, per Statement 42 (balanced distribution).
52. Allow Multi-Cloze with Different Numbered Tags for Semi-Related Concepts: For CD questions testing semi-related concepts (e.g., a term and its characteristic), allow multiple clozes with different numbered tags (e.g., `c1`, `c2`) to obscure elements separately, provided the concepts are tightly linked and justified in metadata, per Statement 18 (cloze specificity). Ensure the question remains concise and uses part-of-speech hints for clarity.
53. Focus on Certification-Relevant Concepts: Ensure questions prioritize core concepts directly tied to the target certification (e.g., CySA+ for cybersecurity), avoiding peripheral course elements like labs, simulators, or broad objectives unless explicitly linked to certification knowledge.
54. Match Distractor Phrasing to Correct Options: In MC questions, ensure distractors closely resemble the correct options in terminology and complexity without using giveaway phrases (e.g., "cyber threats") that make correct answers obvious, using related but distinct cybersecurity tasks instead.
55. Use Plural Phrasing for Compound Answers: When an SA or MC answer includes multiple components (e.g., "monitoring and responding"), use plural phrasing in the question (e.g., "What are key actions...?") to avoid misleading singular cues.
56. Use Certification-Specific Context Markers: Use the certification-specific context marker (e.g., "[CySA]" for CySA+) instead of broader subject terms (e.g., "[Cybersecurity]") in question/text fields and tags to align with the target certification’s focus.
57. When generating Cloze Deletion (CD) questions for processes involving multiple sequential or related actions (e.g., identifying, assessing, addressing in vulnerability management), include all key actions and critical terms (e.g., security weaknesses) as separate clozes with distinct numbered tags (e.g., `c1`, `c2`, `c3`, `c4`) to ensure comprehensive coverage. Use part-of-speech hints (e.g., `::action`, `::issue`) to clarify each cloze’s role, maintaining atomicity and avoiding overlap, per Statement 18 (cloze specificity) and Statement 52 (multi-cloze for semi-related concepts).
58. Consolidate Distractors for MCQ Conversion: When converting single-correct Multiple Choice (MC) questions to the "MCQ - Multi-Variant Shuffled" note type, aggregate distractors from all instances of the original MC set to meet the minimum requirement of 4 distractors, ensuring plausibility and subject relevance without overlap, per Statement 6 (consistent MC distractors) and Statement 49 (distractors from non-target concepts).
59. Suppress Multi-Correct Cards for Single-Correct Concepts: For MCQ notes derived from single-correct MC questions, set HasTwoCorrects and HasThreeCorrects to empty to suppress Choose-2 and Choose-3 cards when the Corrects field contains only one answer, aligning with the singular nature of the source concept and the single-correct example in the instructions (e.g., "What is a fruit?"), per Statement 4 (uniform question phrasing) and the MCQ note type requirements.
60. Preserve Non-Instance Tags in MCQ Conversion: When converting Multiple Choice (MC) questions to the "MCQ - Multi-Variant Shuffled" note type, preserve all original tags except those indicating specific instances (e.g., `instance-1`, `instance-2`), retaining tags for section, topic, and question type (e.g., `multiple-choice`), per Statement 2 (structured formatting for tags), to maintain traceability and consistency with the source material.
61. Use MCQ Format for All Future Multiple Choice Questions: For all future question-generation tasks, generate Multiple Choice questions exclusively in the "MCQ - Multi-Variant Shuffled" note type, superseding the standard MC format. Ensure compliance with MCQ note type requirements (e.g., at least 3 correct answers and 4 distractors for full compatibility, proper use of HasTwoCorrects and HasThreeCorrects fields), per the guidelines in `pasted-text.txt` and Statement 2 (structured formatting for questions).